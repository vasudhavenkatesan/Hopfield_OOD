{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRdqZpx--Ycs"
      },
      "source": [
        "This is the main code for the paper titled **\"Improving Out-of-Distribution Data Handling and Corruption Resistance via Modern Hopfield Networks\"**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A5hKUCUBVPDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Hopfield/')"
      ],
      "metadata": {
        "id": "omVE2w0SVXgM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install light-the-torch >> /.tmp\n",
        "!ltt install torch torchvision >> /.tmp\n",
        "!pip install fastai --upgrade >> /.tmp"
      ],
      "metadata": {
        "id": "jVuvK81vv7Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hopfield/"
      ],
      "metadata": {
        "id": "5QUbAbA4jceH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5w_cJuPApV2"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "\n",
        "# Store the appropriate device\n",
        "use_cuda = torch.cuda.is_available()\n",
        "use_mps = torch.backends.mps.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G35zcc_3Bcu7"
      },
      "source": [
        "# Training the HopfieldPooling\n",
        "\n",
        "In this section, we train the HopfieldPooling layer on the denoising task. To do so, we utilize the official implementation of the HopfieldPooling layer (https://github.com/ml-jku/hopfield-layers). As a result, it is necessary to clone this repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ngngwdACLNY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ml-jku/hopfield-layers.git\n",
        "!pip3 install git+https://github.com/ml-jku/hopfield-layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSUaRwHtQw9L"
      },
      "outputs": [],
      "source": [
        "!python train_denoising_task.py --save-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFnLqs8BS1vR"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "from train_denoising_task import HopfieldModule\n",
        "hopfieldPooling = HopfieldModule()\n",
        "hopfieldPooling = hopfieldPooling.to(device)\n",
        "hopfieldPooling.load_state_dict(torch.load('models/hop_new.pt', map_location= device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p--jy8OxUNHb"
      },
      "outputs": [],
      "source": [
        "# Loading the training history\n",
        "with open('logs/hopfield_denoise.pkl', 'rb') as f:\n",
        "    hopfield_denoise_history = pickle.load(f)\n",
        "\n",
        "hopfield_denoise_history[\"loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fymnKxcs-dzI"
      },
      "source": [
        "# Training the base model\n",
        "\n",
        "To ensure repeatability, we train and use the default convolutional neural network provided by the official PyTorch repository:\n",
        "\n",
        "https://github.com/pytorch/examples/blob/main/mnist/main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldjibmsF-jKJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Training the baseline model\n",
        "!python conv_mnist.py --save-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpyvHxj7ATwy"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "from conv_mnist import Net\n",
        "baseline = Net()\n",
        "baseline = baseline.to(device)\n",
        "baseline.load_state_dict(torch.load('models/mnist_cnn.pt', map_location= device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4p0g9eKBVrD"
      },
      "source": [
        "# Loading MNIST-C Test Data\n",
        "\n",
        "In this section, we load and visualize the MNIST-C dataset. For this purpose, we used the implementation of `TORCH UNCERTAINTY` with some minor changes to fix some bugs.\n",
        "\n",
        "You can find their official repository here:\n",
        "\n",
        "https://github.com/ENSTA-U2IS-AI/torch-uncertainty/blob/main/torch_uncertainty/datasets/classification/mnist_c.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UxXgOUBT2Ny"
      },
      "outputs": [],
      "source": [
        "import mnist_c\n",
        "\n",
        "# Loading data for all corruptions\n",
        "test_data_all = mnist_c.MNISTC(root=\".\", download=True, split = \"test\", transform=transforms.ToTensor(), subset = \"all\")\n",
        "\n",
        "# Create the test loader for all corruptions\n",
        "test_loader_all = torch.utils.data.DataLoader(test_data_all, batch_size=20, num_workers=1, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rm76jdOVLCk"
      },
      "outputs": [],
      "source": [
        "def visualize_data(data_loader) -> None:\n",
        "    \"\"\"\n",
        "    Helper method to visualize a sample of data.\n",
        "    :param data_loader: The data loader to pull the samples from.\n",
        "    :return: Nothing.\n",
        "    \"\"\"\n",
        "    # Create a plot for four random samples with their labels.\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(6, 6))\n",
        "    # Get a random batch from the data loader.\n",
        "    images, labels = next(iter(data_loader))\n",
        "    # Display each image and label.\n",
        "    for i in range(4):\n",
        "        img = images[i].squeeze()\n",
        "        ax[i // 2, i % 2].imshow(img, cmap=\"gray\")\n",
        "        ax[i // 2, i % 2].axis(\"off\")\n",
        "        ax[i // 2, i % 2].set_title(f\"Label: {labels[i].item()}\")\n",
        "    # Check out the shape of one batch.\n",
        "    print(f\"Shape of a batch images: {images.shape}\")\n",
        "    print(f\"Shape of a batch labels: {labels.shape}\")\n",
        "\n",
        "visualize_data(test_loader_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An5w-bEfVsPa"
      },
      "source": [
        "# The Integration Algorithm\n",
        "\n",
        "In this section, we implement our proposed integration algorithm using the pre-trained `hopfieldPooling` module and `baseline` model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LijRuMNSWrTG"
      },
      "outputs": [],
      "source": [
        "mnistc_subsets = [\n",
        "    \"identity\",\n",
        "    \"brightness\",\n",
        "    \"canny_edges\",\n",
        "    \"dotted_line\",\n",
        "    \"fog\",\n",
        "    \"glass_blur\",\n",
        "    \"impulse_noise\",\n",
        "    \"motion_blur\",\n",
        "    \"rotate\",\n",
        "    \"scale\",\n",
        "    \"shear\",\n",
        "    \"shot_noise\",\n",
        "    \"spatter\",\n",
        "    \"stripe\",\n",
        "    \"translate\",\n",
        "    \"zigzag\",\n",
        "]\n",
        "\n",
        "def test_hop(\n",
        "    basemodel: nn.Module, hop: None | nn.Module, cdae: None | nn.Module, corruption: str\n",
        ") -> None:\n",
        "    test_data = mnist_c.MNISTC(root=\".\", split = \"test\", transform=transforms.ToTensor(), subset = corruption)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, num_workers=1, shuffle = True)\n",
        "    number_use = 0\n",
        "    basemodel.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    number_hop_use = 0\n",
        "    added_time = 0\n",
        "    condition = hop is not None or cdae is not None\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            if condition:\n",
        "\n",
        "                output1 = basemodel(data)\n",
        "\n",
        "                if hop is not None:\n",
        "                    output2 = basemodel(hop(data))\n",
        "                else:\n",
        "                    output2 = basemodel(cdae(data))\n",
        "\n",
        "                prob1, pred1 = output1.max(\n",
        "                    dim=1, keepdim=True\n",
        "                )  # get the index of the max log-probability\n",
        "                prob2, pred2 = output2.max(dim=1, keepdim=True)\n",
        "                mult = prob1 > prob2\n",
        "                pred = (mult * pred1) + ((~mult) * pred2)\n",
        "                number_use += (~mult).sum().item()\n",
        "\n",
        "            else:\n",
        "                output = basemodel(data)\n",
        "                pred = output.argmax(\n",
        "                    dim=1, keepdim=True\n",
        "                )  # get the index of the max log-probability\n",
        "\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"\\nCorruption: {}, {}{} -> Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
        "            corruption,\n",
        "            (\n",
        "                (\", Hopfield-integrated\" if hop is not None else \"CDAE-integrated\")\n",
        "                if condition\n",
        "                else \"not-integrated\"\n",
        "            ),\n",
        "            \", Percentage of Use: ({}/{}:{:0.2f})\".format(number_use,\n",
        "                                                        len(test_loader.dataset),\n",
        "                                                        number_use / len(test_loader.dataset) * 100)\n",
        "            if condition\n",
        "            else \"\",\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if condition:\n",
        "        return correct / len(test_loader.dataset), number_use / len(test_loader.dataset)\n",
        "    else:\n",
        "        return correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8K7g_IIcAnA"
      },
      "outputs": [],
      "source": [
        "acc, acc_hop, hop_usage = {}, {}, {}\n",
        "\n",
        "for sub in mnistc_subsets:\n",
        "  acc[sub] = test_hop(baseline, hop = None, cdae= None, corruption=sub)\n",
        "  acc_hop[sub], hop_usage[sub] = test_hop(baseline, hop = hopfieldPooling, cdae= None, corruption=sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whtDsW33dIYN"
      },
      "outputs": [],
      "source": [
        "# Save logs\n",
        "with open(f'logs/acc.pkl', 'wb') as f:\n",
        "  pickle.dump(acc, f)\n",
        "\n",
        "with open(f'logs/acc_hop.pkl', 'wb') as f:\n",
        "  pickle.dump(acc_hop, f)\n",
        "\n",
        "with open(f'logs/hop_usage.pkl', 'wb') as f:\n",
        "  pickle.dump(hop_usage, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHSicgNPfcK3"
      },
      "outputs": [],
      "source": [
        "# Calculate corruption robustness metrics\n",
        "def calculate_robustness_metrics(acc, acc_integrated):\n",
        "  baseline = {\"corruption_accuracy\": np.mean(np.array([v*100 for k, v in acc.items() if k != \"identity\"])),\n",
        "              \"relative mCE\": 100,\n",
        "              \"mCE\": 100}\n",
        "  integrated = {\"corruption_accuracy\": np.mean(np.array([v*100 for k, v in acc_integrated.items() if k != \"identity\"])),\n",
        "              \"relative mCE\": None,\n",
        "              \"mCE\": None}\n",
        "\n",
        "\n",
        "  def mCE(relative = False):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    for c in acc.keys():\n",
        "      if c != \"identity\":\n",
        "        denominator += (1-acc[c]) - ((1-acc[\"identity\"]) if relative else 0)\n",
        "\n",
        "    for c in acc_integrated.keys():\n",
        "      if c != \"identity\":\n",
        "        numerator += (1-acc_integrated[c]) - ((1-acc_integrated[\"identity\"]) if relative else 0)\n",
        "\n",
        "    return numerator/denominator*100\n",
        "\n",
        "  integrated[\"relative mCE\"] = mCE(relative = True)\n",
        "  integrated[\"mCE\"] = mCE(relative = False)\n",
        "\n",
        "  return baseline, integrated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IevTPvnLhVxA"
      },
      "outputs": [],
      "source": [
        "baseline_metrics, hopfield_integration_metrics = calculate_robustness_metrics(acc, acc_hop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmDGwPylhhjT"
      },
      "outputs": [],
      "source": [
        "baseline_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01A1W0W4hiuX"
      },
      "outputs": [],
      "source": [
        "hopfield_integration_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIlWmTLohkny"
      },
      "source": [
        "# Ablation Study\n",
        "\n",
        "In this part, we replace the HopfieldPooling layer with a stacked Convolutional Denoising Autoencoder (CDAE) and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mAVH4Ssk48f"
      },
      "source": [
        "### Denoising Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK9G-K-Qh-ny"
      },
      "outputs": [],
      "source": [
        "!python train_denoising_task.py --cdae --save-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dnQciNeiMlH"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "from train_denoising_task import CDAE\n",
        "cdae = CDAE()\n",
        "cdae = cdae.to(device)\n",
        "cdae.load_state_dict(torch.load('models/CDAE.pt', map_location= device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go6cqPHfkLB2"
      },
      "outputs": [],
      "source": [
        "# Loading the training history\n",
        "with open('logs/CDAE_denoise.pkl', 'rb') as f:\n",
        "    cdae_denoise_history = pickle.load(f)\n",
        "\n",
        "cdae_denoise_history[\"loss\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaAad3hjJsbn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Compare HopfieldPooling layer and CDAE in terms of MSE for the denoising task\n",
        "plt.figure(figsize=(15, 5), dpi = 200)\n",
        "plt.plot(hopfield_denoise_history[\"loss\"], marker = \"o\", label = \"Hopfield\")\n",
        "plt.plot(cdae_denoise_history[\"loss\"], marker = \"^\", label = \"Autoencoder\")\n",
        "plt.xticks(range(20), labels=[f\"{i}\" for i in range(1, 21)])\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0mBYkaXkuEU"
      },
      "source": [
        "### Integration Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_uAomPwlAtB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "acc_AE, AE_usage = {}, {}\n",
        "\n",
        "for sub in mnistc_subsets:\n",
        "  acc_AE[sub], AE_usage[sub] = test_hop(baseline, hop = None, cdae= cdae, corruption=sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzLFyiFnm3aB"
      },
      "outputs": [],
      "source": [
        "with open(f'acc_AE.pkl', 'wb') as f:\n",
        "  pickle.dump(acc_AE, f)\n",
        "\n",
        "with open(f'AE_usage.pkl', 'wb') as f:\n",
        "  pickle.dump(AE_usage, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATQYHItLlR0D"
      },
      "outputs": [],
      "source": [
        "baseline_metrics, cdae_integration_metrics = calculate_robustness_metrics(acc, acc_AE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uX7por3mOKW"
      },
      "outputs": [],
      "source": [
        "baseline_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDqlT2cKmoWX"
      },
      "outputs": [],
      "source": [
        "cdae_integration_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVjjZ4PUqVSN"
      },
      "source": [
        "### Visualize Output of Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGhrKnDcqeEC"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "corruption = \"fog\"\n",
        "\n",
        "test_data = mnist_c.MNISTC(root=\".\", split = \"test\", transform=transforms.ToTensor(), subset = corruption)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=1, shuffle = True)\n",
        "\n",
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "# get sample outputs\n",
        "output1 = hopfieldPooling(images)\n",
        "output2 = cdae(images)\n",
        "\n",
        "# output is resized into a batch of images\n",
        "output1 = output1.view(batch_size, 1, 28, 28)\n",
        "output2 = output2.view(batch_size, 1, 28, 28)\n",
        "# use detach when it's an output that requires_grad\n",
        "output1 = output1.cpu().detach().numpy()\n",
        "output2 = output2.cpu().detach().numpy()\n",
        "\n",
        "# plot the first seven input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=3, ncols=7, sharex=True, sharey=True, figsize=(10,4), dpi = 200)\n",
        "\n",
        "r, c = 0, 0\n",
        "y_labels = [\"Corrupted\", \"Hopfield\", \"Autoencoder\"]\n",
        "# input images on top row, reconstructions on bottom\n",
        "for img, row in zip([images.cpu(), output1, output2], axes):\n",
        "    c = 0\n",
        "    for i, (img, ax) in enumerate(zip(img, row)):\n",
        "        if not r:\n",
        "           ax.set_title(f\"True label: {labels[i].item()}\")\n",
        "        ax.imshow(np.squeeze(img), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.set_yticks([])\n",
        "        if not c:\n",
        "          ax.set_ylabel(y_labels[r])\n",
        "        c+=1\n",
        "    r+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTmNDXFsMt0Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}